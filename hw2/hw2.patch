diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index fe102cd2e..a583a70db 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -688,7 +688,25 @@ static void check_vcpu_requests(struct kvm_vcpu *vcpu)
 		 * Clear IRQ_PENDING requests that were made to guarantee
 		 * that a VCPU sees new virtual interrupts.
 		 */
-		kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+		if(kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu)){
+			/**
+			 * vm class
+			 * kvm_vcpu_write_guest 變更 vcpu (存數據) should be execute in check_vcpu_requests
+			 */
+			if (vm_class_is_trigger == 1 && vm_class_target_vcpu == vcpu) {
+				int ret = kvm_vcpu_write_guest(vcpu, vm_class_gpa, vm_class_data, vm_class_size);
+				if(ret < 0){
+					printk("kvm error: error code for write vm mem %d \n", ret);
+					printk("kvm error: can not write to guest mem %llx \n", vm_class_gpa);
+				} else {
+					printk("kvm info: set mem in vm success\n");
+				}
+				vm_class_target_vcpu = 0;
+				vm_class_is_trigger = 0;
+				vm_class_gpa = 0;
+				vm_class_size = 0;
+			}
+		}
 
 		if (kvm_check_request(KVM_REQ_RECORD_STEAL, vcpu))
 			kvm_update_stolen_time(vcpu);
@@ -704,6 +722,7 @@ static void check_vcpu_requests(struct kvm_vcpu *vcpu)
 		if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu))
 			kvm_pmu_handle_pmcr(vcpu,
 					    __vcpu_sys_reg(vcpu, PMCR_EL0));
+		
 	}
 }
 
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index a067410eb..3c83f9f71 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -863,6 +863,25 @@ struct kvm_ppc_resize_hpt {
 #define KVM_VM_TYPE_ARM_IPA_SIZE_MASK	0xffULL
 #define KVM_VM_TYPE_ARM_IPA_SIZE(x)		\
 	((x) & KVM_VM_TYPE_ARM_IPA_SIZE_MASK)
+
+/**
+ * vm class
+ */
+extern struct kvm_vcpu *vm_class_target_vcpu;
+extern int vm_class_is_trigger;
+extern uint64_t vm_class_gpa;
+extern char vm_class_data[1024];
+extern uint64_t vm_class_size;
+
+struct kvm_arm_write_gpa_args {
+       uint32_t vmid;  // the vmid that you, as the host, want to write to
+       uint64_t gpa;   // the gpa of the guest
+       uint64_t data;  // address of the payload in host user space
+       uint64_t size;  // size of the payload
+};
+
+#define KVM_ARM_WRITE_GPA         _IOW(KVMIO,   0xfe, struct kvm_arm_write_gpa_args)
+
 /*
  * ioctls for /dev/kvm fds:
  */
diff --git a/tools/include/uapi/linux/kvm.h b/tools/include/uapi/linux/kvm.h
index a067410eb..f1edc2195 100644
--- a/tools/include/uapi/linux/kvm.h
+++ b/tools/include/uapi/linux/kvm.h
@@ -863,6 +863,19 @@ struct kvm_ppc_resize_hpt {
 #define KVM_VM_TYPE_ARM_IPA_SIZE_MASK	0xffULL
 #define KVM_VM_TYPE_ARM_IPA_SIZE(x)		\
 	((x) & KVM_VM_TYPE_ARM_IPA_SIZE_MASK)
+
+/**
+ * vm class
+ */
+struct kvm_arm_write_gpa_args {
+       uint32_t vmid;  // the vmid that you, as the host, want to write to
+       uint64_t gpa;   // the gpa of the guest
+       uint64_t data;  // address of the payload in host user space
+       uint64_t size;  // size of the payload
+};
+
+#define KVM_ARM_WRITE_GPA         _IOW(KVMIO,   0xfe, struct kvm_arm_write_gpa_args)
+
 /*
  * ioctls for /dev/kvm fds:
  */
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 7851f3a1b..4ed88b463 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2914,7 +2914,9 @@ int kvm_vcpu_write_guest(struct kvm_vcpu *vcpu, gpa_t gpa, const void *data,
 	int seg;
 	int offset = offset_in_page(gpa);
 	int ret;
-
+	printk("kvm info: gfn %llx \n", gfn);
+	printk("kvm info: offset %d \n", offset);
+	printk("kvm info: len %ld \n", len);
 	while ((seg = next_segment(len, offset)) != 0) {
 		ret = kvm_vcpu_write_guest_page(vcpu, gfn, data, offset, seg);
 		if (ret < 0)
@@ -4653,6 +4655,47 @@ static int kvm_dev_ioctl_create_vm(unsigned long type)
 	return r;
 }
 
+/**
+ * vm class
+*/
+struct kvm_vcpu *vm_class_target_vcpu = 0;
+int vm_class_is_trigger = 0;
+uint64_t vm_class_gpa = 0;
+char vm_class_data[1024];
+uint64_t vm_class_size = 0;
+
+// 展示所有 vmid
+static void show_vm_vid(void)
+{
+	struct kvm *kvm;
+	mutex_lock(&kvm_lock);
+	printk("show kvm vmid start:\n");
+	list_for_each_entry (kvm, &vm_list, vm_list) {
+		printk("kvm vmid : %d\n", kvm->arch.mmu.vmid.vmid);
+	}
+	printk("show kvm vmid end!\n");
+	mutex_unlock(&kvm_lock);
+
+	return;
+}
+
+// 獲取 id 符合的 VM pointer
+static struct kvm * get_vm_by_id(uint32_t vmid)
+{
+	struct kvm *kvm;
+	struct kvm *target_vm = 0;
+
+	mutex_lock(&kvm_lock);
+	list_for_each_entry (kvm, &vm_list, vm_list) {
+		if (kvm->arch.mmu.vmid.vmid == vmid) {
+			target_vm = kvm;
+		}
+	}
+	mutex_unlock(&kvm_lock);
+
+	return target_vm;
+}
+
 static long kvm_dev_ioctl(struct file *filp,
 			  unsigned int ioctl, unsigned long arg)
 {
@@ -4664,6 +4707,33 @@ static long kvm_dev_ioctl(struct file *filp,
 			goto out;
 		r = KVM_API_VERSION;
 		break;
+	/**
+ 	 * vm class
+ 	 */
+	case KVM_ARM_WRITE_GPA:
+	{
+		struct kvm_arm_write_gpa_args *gpa_para_struct = (struct kvm_arm_write_gpa_args*)(void *)arg;
+		struct kvm *target_vm = get_vm_by_id(gpa_para_struct->vmid);
+		if (!target_vm) {
+			printk("kvm error: can not find target vm\n");
+			show_vm_vid();
+			r = -EINVAL;
+			break;
+		}
+		// translate parameter
+		vm_class_target_vcpu = target_vm->vcpus[0];
+		vm_class_is_trigger = 1;
+		vm_class_gpa = gpa_para_struct->gpa;
+		vm_class_size = gpa_para_struct->size;
+		memcpy(vm_class_data, gpa_para_struct->data, vm_class_size);
+		// 用 kvm_make_request 創造中斷, request CPU pending (other ioctl req will use "check_vcpu_requests" than handle write guset in it)
+		kvm_make_request(KVM_REQ_IRQ_PENDING, vm_class_target_vcpu);
+		// 用 kvm_vcpu_kick 叫醒 CPU (optional 可以被動等待 VMM 叫醒)
+		kvm_vcpu_kick(vm_class_target_vcpu);
+		printk("kvm success: update vm end (pending, wait for checked)\n");
+		r = 0;
+		break;
+	}
 	case KVM_CREATE_VM:
 		r = kvm_dev_ioctl_create_vm(arg);
 		break;
